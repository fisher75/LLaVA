[2024-04-22 03:43:13,255] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 03:43:28,314] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-04-22 03:43:28,314] [INFO] [runner.py:571:main] cmd = /home/users/ntu/chih0001/anaconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path /home/users/ntu/chih0001/scratch/model/llava-v1.6-vicuna-7b --version v1 --data_path /home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json --image_folder ./playground/data --vision_tower /home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /home/users/ntu/chih0001/scratch/model/lora/llava-v1.6-vicuna-7b-DSLLM-lora --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2024-04-22 03:43:31,655] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 03:43:35,116] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-04-22 03:43:35,116] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-22 03:43:35,116] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-22 03:43:35,116] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-22 03:43:35,116] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-04-22 03:43:45,842] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 03:43:45,843] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 03:43:45,844] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 03:43:45,844] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-22 03:43:56,196] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-22 03:43:56,196] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-22 03:43:56,196] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-22 03:43:56,196] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-22 03:43:56,196] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2024-04-22 03:44:17,345] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 687, num_elems = 7.06B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.82s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.83s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.84s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.04s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.04s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.04s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.65s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.30s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  5.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.56s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  5.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.56s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  5.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.56s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.64s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.69s/it]
Adding LoRA adapters...
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Formatting inputs...Skip in lazy mode
Traceback (most recent call last):
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train.py", line 961, in train
    trainer = LLaVATrainer(model=model,
  File "/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 367, in __init__
    self.create_accelerator_and_postprocess()
  File "/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 4127, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(
TypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'
Traceback (most recent call last):
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train.py", line 961, in train
    trainer = LLaVATrainer(model=model,
  File "/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 367, in __init__
    self.create_accelerator_and_postprocess()
  File "/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 4127, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(
TypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'
Traceback (most recent call last):
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train.py", line 961, in train
    trainer = LLaVATrainer(model=model,
  File "/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 367, in __init__
    self.create_accelerator_and_postprocess()
  File "/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 4127, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(
TypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'
Traceback (most recent call last):
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train.py", line 961, in train
    trainer = LLaVATrainer(model=model,
  File "/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 367, in __init__
    self.create_accelerator_and_postprocess()
  File "/home/users/ntu/chih0001/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 4127, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(
TypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'
[2024-04-22 03:45:03,232] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2188845
[2024-04-22 03:45:03,463] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2188846
[2024-04-22 03:45:03,480] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2188847
[2024-04-22 03:45:03,480] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2188848
[2024-04-22 03:45:03,587] [ERROR] [launch.py:321:sigkill_handler] ['/home/users/ntu/chih0001/anaconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/home/users/ntu/chih0001/scratch/model/llava-v1.6-vicuna-7b', '--version', 'v1', '--data_path', '/home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json', '--image_folder', './playground/data', '--vision_tower', '/home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/users/ntu/chih0001/scratch/model/lora/llava-v1.6-vicuna-7b-DSLLM-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2024-04-22 03:45:06.349468:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 6790537.pbs101
	Project: 12002486
	Exit Status: 1
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(64), Used(64)
	CPU Time Used: 00:48:27
	Memory: Requested(440gb), Used(14518328kb)
	Vmem Used: 239334904kb
	Walltime: Requested(48:00:00), Used(00:02:18)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (x1000c0s6b0n1:ngpus=4:ncpus=64:mem=461373440kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	GPU Duration: 2.32mins
	GPU Energy Consumed: 65.69W
	GPU Max GPU Memory Used: 64.98GB
	Memory Throughput Rate (Average): x1000c0s6b0n1:(gpu1:0%+gpu3:0%+gpu0:0%+gpu2:0%)
	Memory Throughput Rate (Max): x1000c0s6b0n1:(gpu1:1%+gpu3:1%+gpu0:1%+gpu2:1%)
	Memory Throughput Rate (Min): x1000c0s6b0n1:(gpu1:0%+gpu3:0%+gpu0:0%+gpu2:0%)
	GPU SM Utilization (Average): x1000c0s6b0n1:(gpu1:7%+gpu3:8%+gpu0:0%+gpu2:7%)
	GPU SM Utilization (Max): x1000c0s6b0n1:(gpu1:100%+gpu3:100%+gpu0:10%+gpu2:100%)
	GPU SM Utilization (Min): x1000c0s6b0n1:(gpu1:0%+gpu3:0%+gpu0:0%+gpu2:0%)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Warning: GPUs 0 have a percentage of 0 utilisation.
GPU application profile: Low
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

